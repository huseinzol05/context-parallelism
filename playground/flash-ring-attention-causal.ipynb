{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc582a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338574b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn.flash_attn_interface import (\n",
    "    _flash_attn_varlen_forward,\n",
    "    _flash_attn_varlen_backward,\n",
    ")\n",
    "from typing import Tuple, Optional\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1343092",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_num = 16\n",
    "dim = 128\n",
    "seq_len = 100\n",
    "chunk_size = 5\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284415e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqlen, 3, nheads, d\n",
    "q = torch.randn(seq_len, head_num, dim).cuda().to(torch.bfloat16)\n",
    "k = torch.randn(seq_len, head_num, dim).cuda().to(torch.bfloat16)\n",
    "v = torch.randn(seq_len, head_num, dim).cuda().to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ff4cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.ones((batch_size, seq_len))\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e5edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_seqlens = torch.tensor([  0, 100], dtype = torch.int32).cuda()\n",
    "block_out_full, _, _, _, _, _, _, _ = _flash_attn_varlen_forward(\n",
    "    q, k, v, cu_seqlens, cu_seqlens, 100, 100, 0.0, 1.0,\n",
    "                          causal = True, window_size=(-1, -1),\n",
    "    alibi_slopes=None, return_softmax = False, block_table = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "788731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def flatten_varlen_lse(lse, cu_seqlens):\n",
    "    new_lse = []\n",
    "    for i in range(len(cu_seqlens) - 1):\n",
    "        start, end = cu_seqlens[i], cu_seqlens[i + 1]\n",
    "        new_lse.append(lse[i, :, : end - start])\n",
    "    return torch.cat(new_lse, dim=1)\n",
    "\n",
    "def _update_out_and_lse(\n",
    "    out: torch.Tensor,\n",
    "    lse: torch.Tensor,\n",
    "    block_out: torch.Tensor,\n",
    "    block_lse: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    block_out = block_out.to(torch.float32)\n",
    "    block_lse = block_lse.transpose(-2, -1).unsqueeze(dim=-1)\n",
    "\n",
    "    # new_lse = lse + torch.log(1 + torch.exp(block_lse - lse))\n",
    "    # torch.exp(lse - new_lse) * out + torch.exp(block_lse - new_lse) * block_out\n",
    "    # For additional context and discussion, please refer to:\n",
    "    # https://github.com/zhuzilin/ring-flash-attention/pull/34#issuecomment-2076126795\n",
    "    out = out - F.sigmoid(block_lse - lse) * (out - block_out)\n",
    "    lse = lse - F.logsigmoid(lse - block_lse)\n",
    "\n",
    "    return out, lse\n",
    "\n",
    "def update_out_and_lse(\n",
    "    out: Optional[torch.Tensor],\n",
    "    lse: Optional[torch.Tensor],\n",
    "    block_out: torch.Tensor,\n",
    "    block_lse: torch.Tensor,\n",
    "    slice_=None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if out is None:\n",
    "        if slice_ is not None:\n",
    "            raise RuntimeError(\"first update_out_and_lse should not pass slice_ args\")\n",
    "        out = block_out.to(torch.float32)\n",
    "        lse = block_lse.transpose(-2, -1).unsqueeze(dim=-1)\n",
    "    elif slice_ is not None:\n",
    "        slice_out, slice_lse = out[slice_], lse[slice_]\n",
    "        slice_out, slice_lse = _update_out_and_lse(\n",
    "            slice_out, slice_lse, block_out, block_lse\n",
    "        )\n",
    "        out[slice_], lse[slice_] = slice_out, slice_lse\n",
    "    else:\n",
    "        out, lse = _update_out_and_lse(out, lse, block_out, block_lse)\n",
    "    return out, lse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81d222",
   "metadata": {},
   "source": [
    "## Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d218c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_chunks = q.chunk(chunk_size, dim = 0)\n",
    "k_chunks = k.chunk(chunk_size, dim = 0)\n",
    "v_chunks = v.chunk(chunk_size, dim = 0)\n",
    "seq_chunk = seq_len // chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ce9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = None\n",
    "lse = None\n",
    "\n",
    "q_ = q_chunks[0]\n",
    "cu_seqlens = torch.tensor([  0, 20], dtype = torch.int32).cuda()\n",
    "\n",
    "for i in range(len(q_chunks)):\n",
    "    block_out, _, _, _, _, block_lse, _, _ = _flash_attn_varlen_forward(\n",
    "        q_, k_chunks[i], v_chunks[i], cu_seqlens, cu_seqlens, 100, 100, 0.0, 1.0,\n",
    "                          causal = True and i == 0, window_size=(-1, -1),\n",
    "    alibi_slopes=None, return_softmax = False, block_table = None)\n",
    "    if block_lse.dim() == 3:\n",
    "        old_lse = True\n",
    "        block_lse = flatten_varlen_lse(\n",
    "            block_lse,\n",
    "            cu_seqlens=cu_seqlens,\n",
    "        )\n",
    "    out, lse = update_out_and_lse(out, lse, block_out, block_lse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05eb6cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 35,  25, 111,  18,  75,  96,  89,  51,  56,  48, 125,  10, 116,  11,\n",
       "          38,  72],\n",
       "        [ 27,  25, 122,  18,  75,  96,  89, 102,  81,  93, 110,  10,  47,  63,\n",
       "          38,  72],\n",
       "        [ 35,  96, 122,  18,  80,  65,  85, 119,  56,  97, 110,  10,   7,  63,\n",
       "          71,  21],\n",
       "        [ 82,  96,  34,  92, 114,  19,  27,  51, 121,   4, 112, 126,   7,  21,\n",
       "          71,  66],\n",
       "        [ 27, 107,  88,  99,  75,  14,  89, 119,  81,  93,  34,  45,   7,  11,\n",
       "         112,  77],\n",
       "        [ 82,  13,  88,  18,  12,  80,  81, 119,  59,  20,  34,  45, 116,  21,\n",
       "         103,  77],\n",
       "        [ 33, 120, 115,  48,  75,  65, 108, 119, 113,  93,  34,  52, 116,  11,\n",
       "         103,  77],\n",
       "        [ 42, 114,  22,  64,  49,  20, 108,   9,  56,  68, 125,  10,   7,  87,\n",
       "         103, 105],\n",
       "        [ 42, 120, 115,  76,  64, 117,  27, 102,  59, 110,  34,  14,  47,  88,\n",
       "         112,  72],\n",
       "        [ 42,  18,  34,  92,  77, 121,  89,  36, 106,  93,  90,  75,  47,  33,\n",
       "         126,  72],\n",
       "        [ 72, 120,  26,  56,  39,  14,  54, 111, 107,  45,  84, 126,  27,  63,\n",
       "          80,  21],\n",
       "        [ 55,  30,  88,  99,  51,  80,  24,   9,  65,  48,  34,  52,  61,  65,\n",
       "          61,  25],\n",
       "        [100,  72, 113,   9,  97,  85, 108, 119,  41,  93,  65, 126,   7,  88,\n",
       "         103,  86],\n",
       "        [ 28,  60,  88,  56, 120,  81,   4,   0, 121,  71,  51,  39,  47, 111,\n",
       "          38,  99],\n",
       "        [ 82,  77,  61,  76, 126,  78,  88,   9,  43,   8, 124,   2, 118,  19,\n",
       "          50,  78],\n",
       "        [ 82,  87,  76,   9,  85,  72,  92, 109,  66,  48,  75,  12,  47, 111,\n",
       "         109,  43],\n",
       "        [ 42, 104, 127,  57,  64,  14,  69,   0, 107, 111,  90, 126,  48,  52,\n",
       "          97,  75],\n",
       "        [ 27,  58,  61,  11,  10,  20,  97, 119,  33,  93,  32,  39,  85,  90,\n",
       "           6,  25],\n",
       "        [ 33, 119, 107,  76,  35,  70,  60,  27,  41,  85,  75,  74,  85,  48,\n",
       "         103,  66],\n",
       "        [ 65,  77,   0,   9,  61,  72,  10,  90,  20,   8,  48,  38, 104,  11,\n",
       "          83,  21]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_out_full[:20].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a4f5bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 14,  88,  93,   7, 127, 108,  99,  52,  40, 113,  41, 125,  16,  23,\n",
       "         123, 113],\n",
       "        [ 57,  98,  23,  65, 119,   6,  44,  29,  56,  10,  14,  94,  42,  43,\n",
       "          19,  82],\n",
       "        [ 35,  55,  23,  19,  70,  55,  44,  94,  34,  39,   3,  73,  89,  50,\n",
       "          89,  39],\n",
       "        [116,  59,  81,  17,  75,  29,  20,  88,  56,  43,   7, 126,  87,  43,\n",
       "         112,  51],\n",
       "        [ 71,  72,  73,  96,  12, 125,  26,   8,  54,  86, 120,  73,  66, 115,\n",
       "         122,  80],\n",
       "        [  1,  54,  37, 125,  12, 103,  54, 107,  16, 118,   1, 110,   4,  35,\n",
       "          10, 107],\n",
       "        [ 11,  99,  55,  14, 112,  41,  42,  11, 126,  30,  23,  26,  64, 105,\n",
       "           7,  11],\n",
       "        [ 34,  37, 101,  59,  70,  55,  66, 111,  58,  68, 125,  81,   7,  93,\n",
       "          70,   3],\n",
       "        [ 85,  10,  21,  18,  41, 123,  37,  62,  43,  66,  52,  66,  63,  54,\n",
       "          73,  20],\n",
       "        [ 18,  25,   1,  13,  87,  79, 121, 104,  85,  13,  90,   1,  96,  18,\n",
       "          49,  95],\n",
       "        [ 48,  91,   8,   6,  85,  81,  66,  88,  39,  45,  91, 100,  65,  58,\n",
       "          80, 113],\n",
       "        [126,   0,  53,  28,  51,   2,  95, 119,  50,  37,  86,  76,  52, 105,\n",
       "           3,  25],\n",
       "        [ 79,  83, 113, 111,  87,  85,  79, 119,   4,  93,   7, 118, 113,  88,\n",
       "          26,  86],\n",
       "        [ 99,   3,   2,  37, 120,  32,  44,  10,   4,  47,   1,  71, 104,  63,\n",
       "         114, 117],\n",
       "        [101, 125,  61,  90,  28, 119,  85,  91, 120,  40,  59,  75,  88, 112,\n",
       "         110,  11],\n",
       "        [ 82, 125,  37,   9,  53,  29,  32, 109,  52, 111, 104, 126,   7,  29,\n",
       "          20,  92],\n",
       "        [ 42,   3,  20,  11,  64, 118,  36,  61,  51,   8,  59,  74,  48,  77,\n",
       "          69,  28],\n",
       "        [ 63,  55, 122,  85,  80,  19,  79,  18,  34,  70,   7,  24,  96,  33,\n",
       "          81,  25],\n",
       "        [ 80,  90,   8,  86,  83,  57, 124,  48,   4,  85,  18,  76,  71,  48,\n",
       "         103,  10],\n",
       "        [ 97, 102,  47,  25,  61, 123,  62,  48,  67,  45,  56,  38,   7,  35,\n",
       "           2,  39]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c14b5adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0969, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((block_out_full[:20].argmax(-1) == out.argmax(-1)).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848a7c9",
   "metadata": {},
   "source": [
    "## Non Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be659ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_seqlens = torch.tensor([  0, 100], dtype = torch.int32).cuda()\n",
    "block_out_full_noncausal, _, _, _, _, _, _, _ = _flash_attn_varlen_forward(\n",
    "    q, k, v, cu_seqlens, cu_seqlens, 100, 100, 0.0, 1.0,\n",
    "                          causal = False, window_size=(-1, -1),\n",
    "    alibi_slopes=None, return_softmax = False, block_table = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "760393ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = None\n",
    "lse = None\n",
    "\n",
    "q_ = q_chunks[0]\n",
    "cu_seqlens = torch.tensor([  0, 20], dtype = torch.int32).cuda()\n",
    "\n",
    "for i in range(len(q_chunks)):\n",
    "    block_out, _, _, _, _, block_lse, _, _ = _flash_attn_varlen_forward(\n",
    "        q_, k_chunks[i], v_chunks[i], cu_seqlens, cu_seqlens, 100, 100, 0.0, 1.0,\n",
    "                          causal = False, window_size=(-1, -1),\n",
    "    alibi_slopes=None, return_softmax = False, block_table = None)\n",
    "    if block_lse.dim() == 3:\n",
    "        old_lse = True\n",
    "        block_lse = flatten_varlen_lse(\n",
    "            block_lse,\n",
    "            cu_seqlens=cu_seqlens,\n",
    "        )\n",
    "    out, lse = update_out_and_lse(out, lse, block_out, block_lse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ee52ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 14,  96,  26,  76, 127, 108,  99,  52,  40, 113,  41, 125,  16,  23,\n",
       "         123,  60],\n",
       "        [ 57,  98,  23, 115, 119,  19,  44,  29,  56,  10,  14,  94,  42,  43,\n",
       "          19,  82],\n",
       "        [ 41,  15,  23,  19,  70,  55,  44,  94,  34,   8,  32,  73,  21,  50,\n",
       "          89,  39],\n",
       "        [116,  59,  81,  17,  75,  29,  20,  88,  56,  43,   7, 126,  87,  43,\n",
       "         112,  51],\n",
       "        [ 71,  72,  73,  96,  12, 125,  26,   8,  54,  86, 120,  73,  66, 115,\n",
       "         122,  80],\n",
       "        [  1,  54,  37, 125,  12, 103,  54, 107,  16, 118,   1, 110,  61,  47,\n",
       "          10, 107],\n",
       "        [ 11,  99,  55,  14,  17,  41,  42,  11,   4,  30,  23,  26,  64, 105,\n",
       "           7,  11],\n",
       "        [ 34, 100, 101,  59,  70,  55,  66, 111,  58,  68, 125,  81,   7,  93,\n",
       "          70,   3],\n",
       "        [ 85,  10,  21,  18,  41, 123,  37,  62,  43,  66,  52,  66,  63,  54,\n",
       "          73,  20],\n",
       "        [ 18,  25,   1,  13,  87,  79,  74, 104,  34,  13,  90,   1,  96,  18,\n",
       "          49,  95],\n",
       "        [ 48,  91,   8,   6,  85,  81,  66,  88,  39,  45,  91, 100,  65,  58,\n",
       "          80, 113],\n",
       "        [126,   0,  53,  28,  51,  80,  95, 119,  50,  37,  86,  30,  52, 105,\n",
       "           3,  25],\n",
       "        [ 79,  83, 113, 111,  87,  85,  79, 119,   4,  93,   7, 118, 113,  88,\n",
       "          26,  86],\n",
       "        [ 99,   3,   2,  37, 120,  32,  44,  10,   4,  47,   1,  71, 104,  63,\n",
       "         114, 117],\n",
       "        [101, 125,  61,  90,  28, 119,  85,  91, 120,  40, 115,  75,  88, 112,\n",
       "         110,  11],\n",
       "        [ 82, 125,  37,   9,  53,  29,  32, 109,  52, 111, 104, 126,   7,  29,\n",
       "          20,  92],\n",
       "        [ 42,   3,  20,  20,  64, 118,  36,  61,  51,   8,  59,  74,  48,  77,\n",
       "          60,  28],\n",
       "        [ 63,  55, 122,  85,  80,  19,  79,  18,  34,  70,  27,  24,  96,  33,\n",
       "          81,  25],\n",
       "        [ 80,  90,   8,  86,  83,  57, 124,  48,   4,  85,  18,  76,  71,  48,\n",
       "         103,  10],\n",
       "        [ 97, 102,  47,  25,  61, 123,  62,  48,  67,  45,  56,  38,   7,  35,\n",
       "           2,  39]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_out_full_noncausal[:20].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be78af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 14,  96,  26,  76, 127, 108,  99,  52,  40, 113,  41, 125,  16,  23,\n",
       "         123,  60],\n",
       "        [ 57,  98,  23, 115, 119,  19,  44,  29,  56,  10,  14,  94,  42,  43,\n",
       "          19,  82],\n",
       "        [ 41,  15,  23,  19,  70,  55,  44,  94,  34,   8,  32,  73,  21,  50,\n",
       "          89,  39],\n",
       "        [116,  59,  81,  17,  75,  29,  20,  88,  56,  43,   7, 126,  87,  43,\n",
       "         112,  51],\n",
       "        [ 71,  72,  73,  96,  12, 125,  26,   8,  54,  86, 120,  73,  66, 115,\n",
       "         122,  80],\n",
       "        [  1,  54,  37, 125,  12, 103,  54, 107,  16, 118,   1, 110,  61,  47,\n",
       "          10, 107],\n",
       "        [ 11,  99,  55,  14,  17,  41,  42,  11, 126,  30,  23,  26,  64, 105,\n",
       "           7,  11],\n",
       "        [ 34, 100, 101,  59,  70,  55,  66, 111,  58,  68, 125,  81,   7,  93,\n",
       "          70,   3],\n",
       "        [ 85,  10,  21,  18,  41, 123,  37,  62,  43,  66,  52,  66,  63,  54,\n",
       "          73,  20],\n",
       "        [ 18,  25,   1,  13,  87,  79,  74, 104,  34,  13,  90,   1,  96,  18,\n",
       "          49,  95],\n",
       "        [ 48,  91,   8,   6,  85,  81,  66,  88,  39,  45,  91, 100,  65,  58,\n",
       "          80, 113],\n",
       "        [126,   0,  53,  28,  51,  80,  95, 119,  50,  37,  86,  30,  52, 105,\n",
       "           3,  25],\n",
       "        [ 79,  83, 113, 111,  87,  85,  79, 119,   4,  93,   7, 118, 113,  88,\n",
       "          26,  86],\n",
       "        [ 99,   3,   2,  37, 120,  32,  44,  10,   4,  47,   1,  71, 104,  63,\n",
       "         114, 117],\n",
       "        [101, 125,  61,  90,  28, 119,  85,  91, 120,  40, 115,  75,  88, 112,\n",
       "         110,  11],\n",
       "        [ 82, 125,  37,   9,  53,  29,  32, 109,  52, 111, 104, 126,   7,  29,\n",
       "          20,  92],\n",
       "        [ 42,   3,  20,  20,  64, 118,  36,  61,  51,   8,  59,  74,  48,  77,\n",
       "          60,  28],\n",
       "        [ 63,  55, 122,  85,  80,  19,  79,  18,  34,  70,  27,  24,  96,  33,\n",
       "          81,  25],\n",
       "        [ 80,  90,   8,  86,  83,  57, 124,  48,   4,  85,  18,  76,  71,  48,\n",
       "         103,  10],\n",
       "        [ 97, 102,  47,  25,  61, 123,  62,  48,  67,  45,  56,  38,   7,  35,\n",
       "           2,  39]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7cb0022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9969, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((block_out_full_noncausal[:20].argmax(-1) == out.argmax(-1)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba483cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
